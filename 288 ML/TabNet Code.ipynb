{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Categorical columns: ['Label'] \n",
      "\n",
      "Shape of Dataframe:  (2231806, 78) \n",
      "\n",
      "Inspection of Target Feature - Label:\n",
      "\n",
      "Label\n",
      "Benign                        1895314\n",
      "DoS Hulk                       172846\n",
      "DDoS                           128014\n",
      "DoS GoldenEye                   10286\n",
      "FTP-Patator                      5931\n",
      "DoS slowloris                    5385\n",
      "DoS Slowhttptest                 5228\n",
      "SSH-Patator                      3219\n",
      "PortScan                         1956\n",
      "Web Attack � Brute Force         1470\n",
      "Bot                              1437\n",
      "Web Attack � XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def reproducibility_establishment(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_pi = 314159\n",
    "reproducibility_establishment(seed_value=seed_pi)\n",
    "\n",
    "df_data_1 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\Benign-Monday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_2 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\Botnet-Friday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_3 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\Bruteforce-Tuesday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_4 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\DDoS-Friday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_5 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\DoS-Wednesday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_6 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\Infiltration-Thursday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_7 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\Portscan-Friday-no-metadata.parquet', engine='pyarrow')\n",
    "df_data_8 = pd.read_parquet(r'C:\\Users\\hos\\Downloads\\osiriswatch\\dataset\\WebAttacks-Thursday-no-metadata.parquet', engine='pyarrow')\n",
    "\n",
    "df_data = pd.concat([df_data_1, df_data_2, df_data_3, df_data_4, \n",
    "                     df_data_5, df_data_6, df_data_7, df_data_8], axis=0, ignore_index=True)\n",
    "\n",
    "df_data.dropna(inplace=True)\n",
    "df_data.drop_duplicates(inplace=True)\n",
    "df_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Categorical columns:\", df_data.select_dtypes(include=['object']).columns.tolist(), '\\n')\n",
    "print(\"Shape of Dataframe: \", df_data.shape, '\\n')\n",
    "print('Inspection of Target Feature - Label:\\n')\n",
    "print(df_data['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Benign': np.int64(0), 'Bot': np.int64(1), 'DDoS': np.int64(2), 'DoS GoldenEye': np.int64(3), 'DoS Hulk': np.int64(4), 'DoS Slowhttptest': np.int64(5), 'DoS slowloris': np.int64(6), 'FTP-Patator': np.int64(7), 'Heartbleed': np.int64(8), 'Infiltration': np.int64(9), 'PortScan': np.int64(10), 'SSH-Patator': np.int64(11), 'Web Attack � Brute Force': np.int64(12), 'Web Attack � Sql Injection': np.int64(13), 'Web Attack � XSS': np.int64(14)}\n",
      "Class Weights: {np.int64(0): np.float64(0.07850259880805402), np.int64(1): np.float64(103.51601731601731), np.int64(2): np.float64(1.1622775405339723), np.int64(3): np.float64(14.464065672931518), np.int64(4): np.float64(0.8608101783996996), np.int64(5): np.float64(28.45964464847403), np.int64(6): np.float64(27.628191796649336), np.int64(7): np.float64(25.087739808153476), np.int64(8): np.float64(13948.783333333333), np.int64(9): np.float64(4132.972839506173), np.int64(10): np.float64(76.06698477618723), np.int64(11): np.float64(46.226291079812206), np.int64(12): np.float64(101.16977938954366), np.int64(13): np.float64(6974.391666666666), np.int64(14): np.float64(228.2009543285617)}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_data['Label'] = df_data['Label'].astype(str)\n",
    "y = label_encoder.fit_transform(df_data['Label'])\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "X = df_data.drop('Label', axis=1)\n",
    "\n",
    "def extractAllSets(X, y, p_train, p_val, p_test, random_state, shuffle=True):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=(1.0 - p_train), random_state=random_state, shuffle=shuffle)\n",
    "    fraction = p_test / (p_val + p_test)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=fraction, random_state=random_state, shuffle=shuffle)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = extractAllSets(X, y, 0.75, 0.10, 0.15, seed_pi)\n",
    "\n",
    "def r_scale(X_train, X_val, X_test):\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "X_train_r, X_val_r, X_test_r, scaler = r_scale(X_train, X_val, X_test)\n",
    "X_train, X_val, X_test = X_train_r, X_val_r, X_test_r\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n",
    "print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "clf_params = dict(\n",
    "    n_d=77,\n",
    "    n_a=77,\n",
    "    n_steps=5,\n",
    "    gamma=1.85,\n",
    "    cat_idxs=[],\n",
    "    cat_dims=[],\n",
    "    cat_emb_dim=[],\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    epsilon=1e-15,\n",
    "    momentum=0.02,\n",
    "    lambda_sparse=0.001,\n",
    "    seed=314159,\n",
    "    clip_value=4.5,\n",
    "    verbose=0,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params={'lr': 0.01},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ExponentialLR,\n",
    "    scheduler_params={'verbose': False, 'gamma': 0.9},\n",
    "    mask_type='sparsemax',\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=len(np.unique(y_train)),\n",
    "    device_name=device,\n",
    "    n_shared_decoder=1,\n",
    "    n_indep_decoder=1,\n",
    "    grouped_features=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_Validation_accuracy = 0.90968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = TabNetClassifier(**clf_params)\n",
    "\n",
    "fit_params = dict(\n",
    "    X_train=X_train.values,\n",
    "    y_train=y_train,\n",
    "    eval_set=[(X_val.values, y_val)],\n",
    "    eval_name=['Validation'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=16384,\n",
    "    virtual_batch_size=1024,\n",
    "    num_workers=0,\n",
    "    weights=class_weight_dict,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "clf.fit(**fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Classifier model 'clf' saved successfully!\n"
     ]
    }
   ],
   "source": [
    "clf_file_path = 'tabnet_clf.zip'\n",
    "torch.save(clf, clf_file_path)\n",
    "print(\"TabNet Classifier model 'clf' saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hos\\AppData\\Local\\Temp\\ipykernel_16256\\1117322110.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clf_0 = torch.load(clf_0_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "save_folder = 'saved_model' \n",
    "clf_0_file_path = os.path.join(save_folder, 'tabnet_clf.zip')\n",
    "clf_0 = torch.load(clf_0_file_path)\n",
    "print(\"Trained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  \n",
    "output_dim = len(np.unique(y_train)) \n",
    "\n",
    "clf_params['input_dim'] = input_dim\n",
    "clf_params['output_dim'] = output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New TabNetClassifier instance created with correct parameters.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "new_clf_0 = TabNetClassifier(**clf_params)\n",
    "print(\"New TabNetClassifier instance created with correct parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New TabNetClassifier instance created with correct parameters.\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1] \n",
    "output_dim = len(np.unique(y_train))  \n",
    "\n",
    "clf_params['input_dim'] = input_dim\n",
    "clf_params['output_dim'] = output_dim\n",
    "\n",
    "clf_params.pop('device_name', None)\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "new_clf_0 = TabNetClassifier(**clf_params)\n",
    "print(\"New TabNetClassifier instance created with correct parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is initialized in new_clf_0.\n",
      "Network attribute found in clf_0.\n",
      "State dictionary loaded into the new model.\n"
     ]
    }
   ],
   "source": [
    "new_clf_0._set_network()\n",
    "print(\"Network is initialized in new_clf_0.\")\n",
    "if not hasattr(clf, 'network'):\n",
    "    clf_file_path = os.path.join(save_folder, 'tabnet_clf_0.zip')\n",
    "    clf = torch.load(clf_file_path)\n",
    "    print(\"Trained model loaded successfully.\")\n",
    "\n",
    "if hasattr(clf, 'network'):\n",
    "    print(\"Network attribute found in clf_0.\")\n",
    "    network = clf.network\n",
    "else:\n",
    "    raise AttributeError(\"No network attribute found in the trained model.\")\n",
    "\n",
    "new_clf_0.network.load_state_dict(network.state_dict())\n",
    "print(\"State dictionary loaded into the new model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied preds_mapper and classes_ to new_clf_0.\n"
     ]
    }
   ],
   "source": [
    "new_clf_0.preds_mapper = clf.preds_mapper\n",
    "new_clf_0.classes_ = clf.classes_\n",
    "print(\"Copied preds_mapper and classes_ to new_clf_0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at saved_model\\tabnet_clf_0.zip\n",
      "TabNet Classifier model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "clf_0_file_path_new = os.path.join(save_folder, 'tabnet_clf_0')\n",
    "\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "new_clf_0.save_model(clf_0_file_path_new)\n",
    "print(\"TabNet Classifier model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_data['Label'] = df_data['Label'].astype(str)  \n",
    "y_encoded = label_encoder.fit_transform(df_data['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping saved successfully!\n",
      "Scaler saved successfully!\n",
      "Classifier parameters saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_folder = 'saved_model' \n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "label_mapping = {str(k): int(v) for k, v in label_mapping.items()}\n",
    "\n",
    "label_mapping_file_path = os.path.join(save_folder, 'label_mapping.json')\n",
    "with open(label_mapping_file_path, 'w') as f:\n",
    "    json.dump(label_mapping, f)\n",
    "\n",
    "print(\"Label mapping saved successfully!\")\n",
    "\n",
    "scaler_file_path = os.path.join(save_folder, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_file_path)\n",
    "print(\"Scaler saved successfully!\")\n",
    "\n",
    "feature_names = list(X_train.columns)\n",
    "if 'Label' in feature_names:\n",
    "    feature_names.remove('Label')\n",
    "    \n",
    "import json\n",
    "\n",
    "with open('saved_model/feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f)\n",
    "\n",
    "clf_params_file_path = os.path.join(save_folder, 'clf_params.pkl')\n",
    "with open(clf_params_file_path, 'wb') as f:\n",
    "    pickle.dump(clf_params, f)\n",
    "print(\"Classifier parameters saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder saved successfully!\n"
     ]
    }
   ],
   "source": [
    "label_encoder_file_path = os.path.join(save_folder, 'label_encoder.pkl')\n",
    "with open(label_encoder_file_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"LabelEncoder saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit parameters saved successfully!\n",
      "Classification Report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    Benign       1.00      0.90      0.95    284298\n",
      "                       Bot       0.05      0.82      0.10       215\n",
      "                      DDoS       0.98      0.93      0.95     19203\n",
      "             DoS GoldenEye       0.72      0.99      0.83      1543\n",
      "                  DoS Hulk       0.87      0.98      0.93     25927\n",
      "          DoS Slowhttptest       0.56      0.98      0.71       784\n",
      "             DoS slowloris       0.51      0.95      0.66       808\n",
      "               FTP-Patator       0.23      0.99      0.37       890\n",
      "                Heartbleed       0.50      1.00      0.67         2\n",
      "              Infiltration       0.01      0.80      0.01         5\n",
      "                  PortScan       0.09      0.76      0.16       293\n",
      "               SSH-Patator       0.35      0.93      0.51       483\n",
      "  Web Attack � Brute Force       0.00      0.17      0.01       220\n",
      "Web Attack � Sql Injection       0.00      0.00      0.00         3\n",
      "          Web Attack � XSS       0.03      0.98      0.05        98\n",
      "\n",
      "                  accuracy                           0.91    334772\n",
      "                 macro avg       0.39      0.81      0.46    334772\n",
      "              weighted avg       0.98      0.91      0.94    334772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[256499   3070    347    582   3637    570    731   2985      2    571\n",
      "    1372    809   9376    632   3115]\n",
      " [    10    177      0      0      0      0      0      0      0      0\n",
      "       0      0     26      0      2]\n",
      " [   431      0  17774     12      6      8      0      0      0      0\n",
      "     972      0      0      0      0]\n",
      " [    15      0      0   1524      0      1      0      0      0      0\n",
      "       0      1      0      0      2]\n",
      " [   219      0      0      0  25504      0      0      0      0      0\n",
      "       1      1      0      0    202]\n",
      " [     4      0      0      0      0    768     12      0      0      0\n",
      "       0      0      0      0      0]\n",
      " [     3      0      0      0      0     33    770      0      0      0\n",
      "       0      0      0      0      2]\n",
      " [     5      0      0      0      0      0      6    878      0      0\n",
      "       0      0      1      0      0]\n",
      " [     0      0      0      0      0      0      0      0      2      0\n",
      "       0      0      0      0      0]\n",
      " [     0      1      0      0      0      0      0      0      0      4\n",
      "       0      0      0      0      0]\n",
      " [    23      0      0      0      8      0      1      0      0      0\n",
      "     222     36      1      0      2]\n",
      " [     4      0      0      0      0      0      0      1      0      0\n",
      "       0    451     21      6      0]\n",
      " [     0      0      0      0      0      0      0      0      0      0\n",
      "       0      0     38      0    182]\n",
      " [     0      0      0      1      0      0      0      0      0      0\n",
      "       0      0      2      0      0]\n",
      " [     0      0      0      1      0      0      0      0      0      0\n",
      "       0      0      1      0     96]]\n"
     ]
    }
   ],
   "source": [
    "fit_params_file_path = os.path.join(save_folder, 'fit_params.pkl')\n",
    "with open(fit_params_file_path, 'wb') as f:\n",
    "    pickle.dump(fit_params, f)\n",
    "print(\"Fit parameters saved successfully!\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test.values)\n",
    "\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
